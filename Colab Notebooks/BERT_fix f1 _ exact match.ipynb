{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOyQFtguCN5KH7f7UzdhQXr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7FDeMEaOa0u5","executionInfo":{"status":"ok","timestamp":1696477706599,"user_tz":-660,"elapsed":12919,"user":{"displayName":"Nazmus Salehin (Sammo)","userId":"02081863791735146867"}},"outputId":"7167d1f7-d5b7-4b0c-f550-b376ba240d3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate\n","  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft\n","  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.5)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: safetensors, xxhash, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets, accelerate, peft\n","Successfully installed accelerate-0.23.0 datasets-2.14.5 dill-0.3.7 huggingface-hub-0.16.4 multiprocess-0.70.15 peft-0.5.0 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.34.0 xxhash-3.4.1\n"]}],"source":["!pip install transformers datasets accelerate -U peft"]},{"cell_type":"code","source":["from transformers import BertTokenizerFast, BertForQuestionAnswering, TrainingArguments, Trainer\n","from datasets import load_dataset\n","from peft import PeftModelForQuestionAnswering, PromptEncoderConfig, PromptEncoder\n","from collections import Counter\n","import re\n","import string\n","\n","# Load the SQuAD v2 dataset\n","dataset = load_dataset('squad_v2')\n","train_dataset = dataset['train']\n","eval_dataset = dataset['validation']\n","\n","# Load the fast tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n","\n","# Tokenization function\n","def tokenize_function(examples):\n","    encodings = tokenizer(\n","        examples['question'],\n","        examples['context'],\n","        truncation=True,\n","        padding='max_length',\n","        max_length=492,\n","        return_offsets_mapping=True\n","    )\n","\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, (context, answer) in enumerate(zip(examples['context'], examples['answers'])):\n","        start_position = None\n","        end_position = None\n","\n","        if answer['answer_start']:\n","            start_idx = answer['answer_start'][0]\n","            end_idx = start_idx + len(answer['text'][0])\n","\n","            offset_mapping = encodings['offset_mapping'][i]\n","\n","            for j, (offset_start, offset_end) in enumerate(offset_mapping):\n","                if offset_start <= start_idx and offset_end > start_idx:\n","                    start_position = j\n","                if offset_start < end_idx and offset_end >= end_idx:\n","                    end_position = j\n","                    break\n","\n","            if start_position is not None and end_position is not None:\n","                start_positions.append(start_position)\n","                end_positions.append(end_position)\n","            else:\n","                start_positions.append(0)\n","                end_positions.append(0)\n","        else:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","\n","    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n","    return encodings\n","\n","small_train_dataset = train_dataset.select(range(1000))\n","small_eval_dataset = eval_dataset.select(range(200))\n","\n","# Tokenize the dataset first\n","train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n","eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)\n","\n","# P-tuning Configuration\n","prompt_encoder_config = PromptEncoderConfig(\n","    peft_type=\"P_TUNING\",\n","    task_type=\"QUESTION_ANS\",\n","    num_virtual_tokens=20,\n","    token_dim=768,\n","    num_transformer_submodules=1,\n","    num_attention_heads=12,\n","    num_layers=12,\n","    encoder_reparameterization_type=\"MLP\",\n","    encoder_hidden_size=768,\n",")\n","\n","prompt_encoder = PromptEncoder(prompt_encoder_config)\n","\n","# Create the model\n","model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n","peft_model = PeftModelForQuestionAnswering(model, prompt_encoder_config)\n","peft_model.print_trainable_parameters()\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    evaluation_strategy=\"epoch\",\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=24,\n","    per_device_eval_batch_size=24,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n",")\n","\n","# Normalization Function\n","def normalize_answer(s):\n","    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","# F1 Score Calculation\n","def f1_score(prediction, ground_truth):\n","    prediction_tokens = normalize_answer(prediction).split()\n","    ground_truth_tokens = normalize_answer(ground_truth).split()\n","    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n","    num_same = sum(common.values())\n","\n","    if num_same == 0:\n","        return 0\n","\n","    precision = 1.0 * num_same / len(prediction_tokens)\n","    recall = 1.0 * num_same / len(ground_truth_tokens)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","\n","    return f1\n","\n","# Exact Match Score Calculation\n","def exact_match_score(prediction, ground_truth):\n","    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n","\n","def compute_metrics(eval_pred):\n","    predictions_tuple, labels_tuple = eval_pred.predictions, eval_pred.label_ids\n","    start_logits, end_logits = predictions_tuple\n","    start_positions, end_positions = labels_tuple\n","\n","    f1 = 0.0\n","    exact_match = 0\n","\n","    for i in range(len(start_positions)):\n","        start_pred = start_logits[i].argmax()\n","        end_pred = end_logits[i].argmax()\n","\n","        pred_ans = tokenizer.decode(eval_dataset[i]['input_ids'][start_pred:end_pred + 1])\n","        true_ans = tokenizer.decode(eval_dataset[i]['input_ids'][start_positions[i]:end_positions[i] + 1])\n","\n","        f1 += f1_score(pred_ans, true_ans)\n","        exact_match += exact_match_score(pred_ans, true_ans)\n","\n","    return {'f1': f1/len(start_positions), 'exact_match': exact_match/len(start_positions)}\n","\n","\n","# Trainer\n","trainer = Trainer(\n","    model=peft_model,  # Use the P-tuning-enhanced model\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Fine-tune\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"id":"-s0xitNXbIeq","outputId":"0db0492b-d125-49f4-c926-5bb54d11e09c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["trainable params: 1,790,212 || all params: 110,681,860 || trainable%: 1.6174393888935368\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='87' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 87/126 02:46 < 01:16, 0.51 it/s, Epoch 2.05/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","      <th>Exact Match</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>6.203984</td>\n","      <td>0.011998</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>6.192621</td>\n","      <td>0.009059</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]}]}